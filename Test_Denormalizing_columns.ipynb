{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_name='DataFull.xlsx'\n",
    "#file_name='Data10_07.xlsx'\n",
    "\n",
    "df_A=pd.DataFrame()\n",
    "df_B=pd.DataFrame()\n",
    "df_C=pd.DataFrame()\n",
    "df_D=pd.DataFrame()\n",
    "\n",
    "#create lists to hold headers & other variables\n",
    "HEADERS = []\n",
    "DATES_EXAM = []\n",
    "HEIGHTS=[]\n",
    "WEIGHTS_REF=[]\n",
    "WEIGHTS=[]\n",
    "BMIS=[]\n",
    "ALS=[]\n",
    "ALS_PARO=[]\n",
    "ALS_SALI =[]\n",
    "ALS_DEGL=[] \n",
    "ALS_ERCI=[] \n",
    "ALS_SGAS=[] \n",
    "ALS_AGAS=[] \n",
    "ALS_HABI=[] \n",
    "ALS_LITD=[]\n",
    "ALS_MARC=[] \n",
    "ALS_ESCA =[]\n",
    "ALS_DYSPNE=[] \n",
    "ALS_ORTHOPNE=[] \n",
    "ALS_INSR=[]\n",
    "DATES_RILUZ=[]\n",
    "DATES_PREVENT=[]\n",
    "CVF_ASSIS_THEO=[]\n",
    "CVL_ASSIS_THEO=[]\n",
    "DATES_DECES=[]\n",
    "All_cols=[]\n",
    "#group variables\n",
    "\n",
    "info=['hash_ipp','SEX','DOB','DIAGPROBA','DATEDIAG','FIRSTSYMPTOM','LIEUDEB','AGE_DEBUT']\n",
    "clinical_measures_1=['DATEXAM','HEIGHT','WEIGHT_REF','WEIGHT','BMI','ALS','ALS_PARO','ALS_SALI','ALS_DEGL','ALS_ERCI','ALS_SGAS','ALS_AGAS','ALS_HABI','ALS_LITD','ALS_MARC','ALS_ESCA','ALS_ALS_dyspne','ALS_ALS_orthopne','ALS_INSR']\n",
    "clinical_measures_2=['DATE_PREVENT_PP','CVF_ASSIS_THEO_PP','CVL_ASSIS_THEO_PP']\n",
    "clinical_measures_riluz_1=['DATDRILU_L1']\n",
    "clinical_measures_deces=['DATEDCD']\n",
    "\n",
    "\n",
    "#Read CSV File\n",
    "df = pd.read_excel(file_name, sheet_name='Sheet1')\n",
    "\n",
    "#create a list of all the columns\n",
    "columns = list(df)\n",
    "\n",
    "#split columns list into headers and other variables\n",
    "for col in columns:\n",
    "    if col.startswith('DATEXAM'):\n",
    "        DATES_EXAM.append(col)\n",
    "    elif  col.startswith('HEIGHT'):\n",
    "        HEIGHTS.append(col)\n",
    "    elif col.startswith('WEIGHT_REF') :\n",
    "        WEIGHTS_REF.append(col)\n",
    "    elif  col.startswith('WEIGHT') :\n",
    "        WEIGHTS.append(col)\n",
    "    elif   col.startswith('BMI'): \n",
    "        BMIS.append(col)\n",
    "    elif  col.startswith('DATDRILU'):\n",
    "        DATES_RILUZ.append(col)\n",
    "    elif col.startswith('DATE_PREVENT'):\n",
    "        DATES_PREVENT.append(col)\n",
    "    elif col.startswith('CVF_ASSIS_THEO'):\n",
    "        CVF_ASSIS_THEO.append(col)\n",
    "    elif col.startswith('CVL_ASSIS_THEO'):\n",
    "        CVL_ASSIS_THEO.append(col)\n",
    "    elif col.startswith('DATEDCD'):\n",
    "        DATES_DECES.append(col)\n",
    "    elif  col.startswith('ALS'):\n",
    "        ##\n",
    "        if col.__contains__('PARO'):\n",
    "            ALS_PARO.append(col)\n",
    "        elif col.__contains__('SALI'):\n",
    "            ALS_SALI.append(col)\n",
    "        elif  col.__contains__('DEGL'):\n",
    "            ALS_DEGL.append(col)\n",
    "        elif  col.__contains__('ERCI'):\n",
    "            ALS_ERCI.append(col)\n",
    "        elif  col.__contains__('SGAS') :\n",
    "            ALS_SGAS.append(col)\n",
    "        elif  col.__contains__('AGAS'):\n",
    "            ALS_AGAS.append(col)\n",
    "        elif col.__contains__('HABI') :\n",
    "            ALS_HABI.append(col)\n",
    "        elif col.__contains__('LITD') :\n",
    "            ALS_LITD.append(col)\n",
    "        elif  col.__contains__('MARC') :\n",
    "            ALS_MARC.append(col)\n",
    "        elif  col.__contains__('ESCA'):\n",
    "            ALS_ESCA.append(col)\n",
    "        elif  col.__contains__('dyspne'):\n",
    "            ALS_DYSPNE.append(col)\n",
    "        elif col.__contains__('orthopne'):\n",
    "            ALS_ORTHOPNE.append(col)\n",
    "        elif col.__contains__('INSR'):\n",
    "            ALS_INSR.append(col)\n",
    "        else :\n",
    "            ALS.append(col)\n",
    "    else:\n",
    "        HEADERS.append(col)\n",
    "\n",
    "#For headers take into account only info \n",
    "\n",
    "HEADERS=list( x for x in info )\n",
    "\n",
    "#group column variables\n",
    "All_cols=[]\n",
    "All_cols.append(DATES_EXAM)\n",
    "All_cols.append(HEIGHTS)\n",
    "All_cols.append(WEIGHTS_REF)\n",
    "All_cols.append(WEIGHTS)\n",
    "All_cols.append(BMIS)\n",
    "All_cols.append(ALS)\n",
    "All_cols.append(ALS_PARO)\n",
    "All_cols.append(ALS_SALI)\n",
    "All_cols.append(ALS_DEGL)\n",
    "All_cols.append(ALS_ERCI)\n",
    "All_cols.append(ALS_SGAS)\n",
    "All_cols.append(ALS_AGAS)\n",
    "All_cols.append(ALS_HABI)\n",
    "All_cols.append(ALS_LITD)\n",
    "All_cols.append(ALS_MARC)\n",
    "All_cols.append(ALS_ESCA)\n",
    "All_cols.append(ALS_DYSPNE)\n",
    "All_cols.append(ALS_ORTHOPNE)\n",
    "All_cols.append(ALS_INSR)\n",
    "All_cols.append(DATES_RILUZ)\n",
    "All_cols.append(DATES_PREVENT)\n",
    "All_cols.append(CVF_ASSIS_THEO)\n",
    "All_cols.append(CVL_ASSIS_THEO)\n",
    "All_cols.append(DATES_DECES)\n",
    "\n",
    "#remove empty lists from All_cols list if exist \n",
    "All_cols = [x for x in All_cols if x != []]\n",
    "\n",
    "#Create a final DF with modified columns\n",
    "for lst in All_cols:\n",
    "    df_x = pd.melt(df,\n",
    "                  id_vars=HEADERS,\n",
    "                  value_vars=lst,\n",
    "                  var_name=lst[0],\n",
    "                  value_name=lst[0]+'_Val')\n",
    "         #Concatenate DataFrames 1\n",
    "    if any(elem in lst for elem in clinical_measures_1):\n",
    "        df_A= pd.concat([df_A, df_x],axis=1)\n",
    "    if any(elem in lst for elem in clinical_measures_riluz_1):\n",
    "        #Concatenate DataFrames 2\n",
    "        df_B= pd.concat([df_B, df_x],axis=1)\n",
    "    if any(elem in lst for elem in clinical_measures_2):\n",
    "        #Concatenate DataFrames 3\n",
    "        df_C= pd.concat([df_C, df_x],axis=1)\n",
    "    if any(elem in lst for elem in clinical_measures_deces):\n",
    "        df_D=pd.concat([df_D,df_x],axis=1)\n",
    "        \n",
    "#Delete duplicate columns\n",
    "df_A= df_A.loc[:, ~df_A.columns.duplicated()]\n",
    "df_B= df_B.loc[:, ~df_B.columns.duplicated()]\n",
    "df_C= df_C.loc[:, ~df_C.columns.duplicated()]\n",
    "df_D=df_D.loc[:, ~df_D.columns.duplicated()]\n",
    "\n",
    "#Write Dataframes to csv\n",
    "df_A.to_csv(\"df_clinical_measures_1_modif.csv\",index=False,encoding='utf-8-sig')\n",
    "#df_B.to_csv(\"df_clinical_measures_1_Date_Riluzole.csv\",index=False,encoding='utf-8-sig')\n",
    "#df_C.to_csv(\"df_clinical_measures_2.csv\",index=False,encoding='utf-8-sig')\n",
    "#df_D.to_csv(\"df_clinical_measures_1_Date_Deces.csv\",index=False,encoding='utf-8-sig')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_name1='df_clinical_measures_1.csv'\n",
    "file_name2='df_clinical_measures_1_Date_Riluzole.csv'\n",
    "file_name3='df_clinical_measures_2.csv'\n",
    "\n",
    "df1 = pd.read_csv (file_name1,encoding='utf-8-sig')\n",
    "df2 = pd.read_csv (file_name2,encoding='utf-8-sig')\n",
    "df3 = pd.read_csv (file_name3,encoding='utf-8-sig')\n",
    "\n",
    "df1=df1.drop(['DATEXAM','HEIGHT','WEIGHT_REF','WEIGHT','BMI','ALS','ALS_PARO','ALS_SALI','ALS_DEGL','ALS_ERCI','ALS_SGAS','ALS_AGAS','ALS_HABI','ALS_LITD','ALS_MARC','ALS_ESCA','ALS_ALS_dyspne','ALS_ALS_orthopne','ALS_INSR'],1)\n",
    "df3=df3.drop(['DATE_PREVENT_PP','CVF_ASSIS_THEO_PP','CVL_ASSIS_THEO_PP'],1)\n",
    "df2=df2.drop(['DATDRILU_L1'],1)\n",
    "\n",
    "df1.to_csv(\"df_clinical_measures_1.csv\",index=False,encoding='utf-8-sig')\n",
    "df2.to_csv(\"df_clinical_measures_1_Date_Riluzole.csv\",index=False,encoding='utf-8-sig')\n",
    "df3.to_csv(\"df_clinical_measures_2.csv\",index=False,encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replace null ipps  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#file_name='DataFull.xlsx'\n",
    "file_name='Data10_07.xlsx'\n",
    "df = pd.read_excel(file_name, sheet_name='Sheet1')\n",
    "df['IPP'] = df['IPP'].astype('str') \n",
    "df['IPP']= df['IPP'].replace(['pasIPP','nan'],np.NaN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "pattern=r'^[0-9]+$'\n",
    "#df['IPP'] = df['IPP'].astype('str') \n",
    "#df.IPP.str.match(pattern,na=False)\n",
    "list_ipps=df[df.IPP.str.match(pattern,na=False)]['IPP'].tolist()\n",
    "count_null_ipps=df['IPP'].shape[0]-len(list_ipps)\n",
    "new_ipps=[]\n",
    "#generate ipp that doesn't match ipp in the list_ipp or new_ipps\n",
    "x=0\n",
    "for x in range (0,count_null_ipps):\n",
    "    while True:\n",
    "        generated_ipp=random.randint(1,5000)\n",
    "        if generated_ipp not in list_ipps and generated_ipp not in new_ipps:\n",
    "            new_ipps.append(str(generated_ipp))\n",
    "            break;\n",
    "    x=x+1\n",
    "\n",
    "#assign patient with null ipp ,the new ipp\n",
    "df.loc[df.IPP.isna(),'IPP'] = new_ipps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "df['hash_ipp']=df['IPP'].str.encode('utf-8').apply(lambda x: hashlib.md5(x).hexdigest())\n",
    "#hashlib.md5('bonjour'.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp = df['IPP'].str.encode('utf-8')\n",
    "#tmp.loc[tmp.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['PATIENT','REF_NAME','REF_ID770S4V3638','REF_SURNAME','NAME','SURNAME','NO','IPP','Column1'],1)\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Data.xlsx',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODE all patients in two files , replace null ipps in two files with generated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "file_name1='DataFull.xlsx'\n",
    "file_name2='Data10_07.xlsx'\n",
    "DF1= pd.read_excel(file_name1, sheet_name='Sheet1')\n",
    "DF2= pd.read_excel(file_name2, sheet_name='Sheet1')\n",
    "DF11=DF1[['IPP','NAME','SURNAME','DOB']]\n",
    "DF22=DF2[['IPP','NAME','SURNAME','DOB']]\n",
    "DF3= pd.concat([DF11, DF22], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DF3['IPP'] = DF3['IPP'].astype('str') \n",
    "DF3['IPP']= DF3['IPP'].replace(['pasIPP','nan'],np.NaN)\n",
    "DF3=DF3.drop_duplicates(subset=['NAME', 'SURNAME','DOB'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "pattern=r'^[0-9]+$'\n",
    "#df['IPP'] = df['IPP'].astype('str') \n",
    "#df.IPP.str.match(pattern,na=False)\n",
    "list_ipps=DF3[DF3.IPP.str.match(pattern,na=False)]['IPP'].tolist()\n",
    "count_null_ipps=DF3['IPP'].shape[0]-len(list_ipps)\n",
    "new_ipps=[]\n",
    "#generate ipp that doesn't match ipp in the list_ipp or new_ipps\n",
    "x=0\n",
    "for x in range (0,count_null_ipps):\n",
    "    while True:\n",
    "        generated_ipp=random.randint(1,5000)\n",
    "        if generated_ipp not in list_ipps and generated_ipp not in new_ipps:\n",
    "            new_ipps.append(str(generated_ipp))\n",
    "            break;\n",
    "    x=x+1\n",
    "\n",
    "#assign patient with null ipp ,the new ipp\n",
    "DF3.loc[DF3.IPP.isna(),'IPP'] = new_ipps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for index3,row3 in DF3.iterrows():\n",
    "    for index1,row1 in DF1.iterrows():\n",
    "        if row3['NAME'] == row1['NAME']  and row3['SURNAME'] == row1['SURNAME'] and row3['DOB'] == row1['DOB'] :\n",
    "            DF1.loc[index1,'IPP'] = row3['IPP'] \n",
    "for index3,row3 in DF3.iterrows():            \n",
    "    for index2,row2 in DF2.iterrows():\n",
    "        if row3['NAME'] == row2['NAME']  and row3['SURNAME'] == row2['SURNAME'] and row3['DOB'] == row2['DOB'] :\n",
    "            DF2.loc[index2,'IPP']=row3['IPP']\n",
    "'''        \n",
    "a = pd.merge(DF1,DF3, left_on=['NAME', 'SURNAME', 'DOB'], right_on=['NAME', 'SURNAME', 'DOB'], suffixes=['_a', '']\n",
    "        )['IPP']\n",
    "b = pd.merge(DF2,DF3, left_on=['NAME', 'SURNAME', 'DOB'], right_on=['NAME', 'SURNAME', 'DOB'], suffixes=['_b', '']\n",
    "        )['IPP']\n",
    "DF1.update(a)\n",
    "DF2.update(b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATIENT</th>\n",
       "      <th>REF_NAME</th>\n",
       "      <th>REF_ID770S4V3638</th>\n",
       "      <th>REF_SURNAME</th>\n",
       "      <th>NAME</th>\n",
       "      <th>SURNAME</th>\n",
       "      <th>SEX</th>\n",
       "      <th>DOB</th>\n",
       "      <th>NO</th>\n",
       "      <th>IPP</th>\n",
       "      <th>...</th>\n",
       "      <th>ALS_V_M96_AGAS</th>\n",
       "      <th>ALS_V_M96_HABI</th>\n",
       "      <th>ALS_V_M96_LITD</th>\n",
       "      <th>ALS_V_M96_MARC</th>\n",
       "      <th>ALS_V_M96_ESCA</th>\n",
       "      <th>ALS_V_M96_ALS_dyspne</th>\n",
       "      <th>ALS_V_M96_ALS_orthopne</th>\n",
       "      <th>ALS_V_M96_INSR</th>\n",
       "      <th>DATEDCD_V_M96</th>\n",
       "      <th>Column1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 1562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [PATIENT, REF_NAME, REF_ID770S4V3638, REF_SURNAME, NAME, SURNAME, SEX, DOB, NO, IPP, ALS, ALS_PARO, ALS_SALI, ALS_DEGL, ALS_ERCI, ALS_SGAS, ALS_AGAS, ALS_HABI, ALS_LITD, ALS_MARC, ALS_ESCA, ALS_ALS_dyspne, ALS_ALS_orthopne, ALS_INSR, DATEDCD, DATEXAM_V_M1, ALS_V_M1, ALS_V_M1_PARO, ALS_V_M1_SALI, ALS_V_M1_DEGL, ALS_V_M1_ERCI, ALS_V_M1_SGAS, ALS_V_M1_AGAS, ALS_V_M1_HABI, ALS_V_M1_LITD, ALS_V_M1_MARC, ALS_V_M1_ESCA, ALS_V_M1_ALS_dyspne, ALS_V_M1_ALS_orthopne, ALS_V_M1_INSR, DATEDCD_V_M1, DATEXAM_V_M2, ALS_V_M2, ALS_V_M2_PARO, ALS_V_M2_SALI, ALS_V_M2_DEGL, ALS_V_M2_ERCI, ALS_V_M2_SGAS, ALS_V_M2_AGAS, ALS_V_M2_HABI, ALS_V_M2_LITD, ALS_V_M2_MARC, ALS_V_M2_ESCA, ALS_V_M2_ALS_dyspne, ALS_V_M2_ALS_orthopne, ALS_V_M2_INSR, DATEDCD_V_M2, DATEXAM_V_M3, ALS_V_M3, ALS_V_M3_PARO, ALS_V_M3_SALI, ALS_V_M3_DEGL, ALS_V_M3_ERCI, ALS_V_M3_SGAS, ALS_V_M3_AGAS, ALS_V_M3_HABI, ALS_V_M3_LITD, ALS_V_M3_MARC, ALS_V_M3_ESCA, ALS_V_M3_ALS_dyspne, ALS_V_M3_ALS_orthopne, ALS_V_M3_INSR, DATEDCD_V_M3, DATEXAM_V_M4, ALS_V_M4, ALS_V_M4_PARO, ALS_V_M4_SALI, ALS_V_M4_DEGL, ALS_V_M4_ERCI, ALS_V_M4_SGAS, ALS_V_M4_AGAS, ALS_V_M4_HABI, ALS_V_M4_LITD, ALS_V_M4_MARC, ALS_V_M4_ESCA, ALS_V_M4_ALS_dyspne, ALS_V_M4_ALS_orthopne, ALS_V_M4_INSR, DATEDCD_V_M4, DATEXAM_V_M5, ALS_V_M5, ALS_V_M5_PARO, ALS_V_M5_SALI, ALS_V_M5_DEGL, ALS_V_M5_ERCI, ALS_V_M5_SGAS, ALS_V_M5_AGAS, ALS_V_M5_HABI, ALS_V_M5_LITD, ALS_V_M5_MARC, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 1562 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF1['IPP'] = DF1['IPP'].astype('str') \n",
    "DF1[ ~ DF1.IPP.str.match(pattern,na=False)]\n",
    "\n",
    "DF2['IPP'] = DF2['IPP'].astype('str') \n",
    "DF2[ ~ DF2.IPP.str.match(pattern,na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "DF1['hash_ipp']=DF1['IPP'].str.encode('utf-8').apply(lambda x: hashlib.md5(x).hexdigest())\n",
    "DF1=DF1.drop(['PATIENT','REF_NAME','REF_ID770S4V3638','REF_SURNAME','NAME','SURNAME','NO','IPP','Column1'],1)\n",
    "cols = DF1.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "DF1 = DF1[cols] \n",
    "\n",
    "DF2['hash_ipp']=DF2['IPP'].str.encode('utf-8').apply(lambda x: hashlib.md5(x).hexdigest())\n",
    "DF2=DF2.drop(['PATIENT','REF_NAME','REF_ID770S4V3638','REF_SURNAME','NAME','SURNAME','NO','IPP','Column1'],1)\n",
    "cols = DF2.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "DF2 = DF2[cols] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF1.to_excel('Data.xlsx',encoding='utf-8-sig',index=False)\n",
    "DF2.to_excel('Data10_07.xlsx',encoding='utf-8-sig',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check if patients with null ipps in second file exists in first file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_name1='DataFull.xlsx'\n",
    "file_name2='Data10_07.xlsx'\n",
    "\n",
    "df1=pd.read_excel(file_name1, sheet_name='Sheet1',encoding='utf-8')\n",
    "df2=pd.read_excel(file_name2,sheet_name='Sheet1',encoding='utf-8')\n",
    "\n",
    "pattern=r'^[0-9]+$'\n",
    "df1['IPP'] = df1['IPP'].astype('str') \n",
    "df1_null_ipps = df1[ ~ df1.IPP.str.match(pattern,na=False)]\n",
    "\n",
    "#df1.loc[df1['IPP'] =='P']\n",
    "#df1.loc[df1['IPP']= and df1['SURNAME']+ and df2['NAME']=]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['IPP'] = df2['IPP'].astype('str') \n",
    "df2_null_ipps=df2[~df2.IPP.str.match(pattern,na=False)]\n",
    "s=0\n",
    "for i,r in df2_null_ipps.iterrows():\n",
    "    for index,row in df1.iterrows():\n",
    "        if r['NAME'] == row['NAME'] and r['SURNAME'] == row['SURNAME'] and r['DOB'] == row['DOB']:\n",
    "            s=s+1\n",
    "            print(s)\n",
    "            print('-----------------------------------------------------------------------------')\n",
    "            print(row[['NAME','SURNAME','IPP','DOB']])\n",
    "\n",
    "df2_null_ipps.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
